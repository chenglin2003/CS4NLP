{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07b5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c387023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('blogtext.csv')\n",
    "\n",
    "# Calculate the number of words in each blog post\n",
    "text_len = df.text.apply(lambda x: len(RegexpTokenizer(r'\\w+').tokenize(x))).to_numpy()\n",
    "\n",
    "# Combine the text length with the dataset\n",
    "df['text_len'] = text_len\n",
    "\n",
    "# Remove posts with less than 100 words\n",
    "df = df[df['text_len'] > 100]\n",
    "\n",
    "# Get the number of posts for each author\n",
    "authors = df['id'].to_numpy()\n",
    "author_id, counts = np.unique(authors, return_counts=True)\n",
    "\n",
    "# Remove authors with less than 10 posts\n",
    "valid_authors = author_id[counts > 10]\n",
    "df = df[df['id'].isin(valid_authors)]\n",
    "\n",
    "# Strip text\n",
    "df['text'] = df['text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5e8fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316594/316594 [02:14<00:00, 2350.58it/s]\n"
     ]
    }
   ],
   "source": [
    "punctuation = ['.', ',', '!', '?', ':', ';', '-', '(', ')', '[', ']', '{', '}', \"'\", '\"']\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "short_word_len = 4\n",
    "\n",
    "# We calculate the stylometric features\n",
    "word_len = np.zeros(len(df))\n",
    "sentence_len = np.zeros(len(df))\n",
    "short_words = np.zeros(len(df))\n",
    "digit_prop = np.zeros(len(df))\n",
    "captialized_prop = np.zeros(len(df))\n",
    "letter_freq = np.zeros((len(df), len(letters)))\n",
    "digit_freq = np.zeros((len(df), len(digits)))\n",
    "punctuation_freq = np.zeros((len(df), len(punctuation)))\n",
    "hapax_legomena = np.zeros(len(df))\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    text = df['text'].iloc[i]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "    word_lengths = [len(word) for word in words]\n",
    "\n",
    "    if len(word_lengths) == 0:\n",
    "        print(f\"Empty word length in post {i}\")\n",
    "        break\n",
    "\n",
    "    if len([len(sentence.split(' ')) for sentence in sentences]) == 0:\n",
    "        print(f\"Empty sentence in post {i}\")\n",
    "        break\n",
    "\n",
    "    word_len[i] = np.mean(word_lengths)\n",
    "    short_words[i] = np.sum([1 for word in words if len(word) < short_word_len])\n",
    "    sentence_len[i] = np.mean([len(sentence.split(' ')) for sentence in sentences])\n",
    "\n",
    "    character_counts = Counter(list(text.lower()))\n",
    "\n",
    "    for j, letter in enumerate(letters):\n",
    "        if letter in character_counts:\n",
    "            letter_freq[i][j] = character_counts[letter]\n",
    "        else:\n",
    "            letter_freq[i][j] = 0\n",
    "\n",
    "    for j, digit in enumerate(digits):\n",
    "        if digit in character_counts:\n",
    "            digit_freq[i][j] = character_counts[digit]\n",
    "        else:\n",
    "            digit_freq[i][j] = 0\n",
    "\n",
    "    for j, punct in enumerate(punctuation):\n",
    "        if punct in character_counts:\n",
    "            punctuation_freq[i][j] = character_counts[punct]\n",
    "        else:\n",
    "            punctuation_freq[i][j] = 0\n",
    "\n",
    "    letter_freq[i] /= np.sum(letter_freq[i]) + 1e-10\n",
    "    digit_freq[i] /= np.sum(digit_freq[i]) + 1e-10\n",
    "    punctuation_freq[i] /= np.sum(punctuation_freq[i]) + 1e-10\n",
    "    \n",
    "    hapax_legomena[i] = len([word for word, count in Counter(words).items() if count == 1])\n",
    "\n",
    "    text_len = df['text_len'].iloc[i]\n",
    "\n",
    "    digit_prop[i] = np.sum([1 for word in words if word.isdigit()]) / text_len\n",
    "    captialized_prop[i] = np.sum([1 for word in words if word[0].isupper()]) / text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb60c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_word_len = np.mean(word_len)\n",
    "mean_sentence_len = np.mean(sentence_len)\n",
    "mean_short_words = np.mean(short_words)\n",
    "mean_hapax_legomena = np.mean(hapax_legomena)\n",
    "\n",
    "std_word_len = np.std(word_len)\n",
    "std_sentence_len = np.std(sentence_len)\n",
    "std_short_words = np.std(short_words)\n",
    "std_hapax_legomena = np.std(hapax_legomena)\n",
    "\n",
    "# Normalize the features\n",
    "word_len = (word_len - mean_word_len) / std_word_len\n",
    "sentence_len = (sentence_len - mean_sentence_len) / std_sentence_len\n",
    "short_words = (short_words - mean_short_words) / std_short_words\n",
    "hapax_legomena = (hapax_legomena - mean_hapax_legomena) / std_hapax_legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d44323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the features to the dataframe\n",
    "df['word_len'] = word_len\n",
    "df['sentence_len'] = sentence_len\n",
    "df['short_words'] = short_words\n",
    "df['digit_prop'] = digit_prop\n",
    "df['captialized_prop'] = captialized_prop\n",
    "df['hapax_legomena'] = hapax_legomena\n",
    "\n",
    "# Add the letter frequency features to the dataframe with the column names\n",
    "for i in range(len(letters)):\n",
    "    df[f'letter_freq_{letters[i]}'] = letter_freq[:, i]\n",
    "for i in range(len(digits)):\n",
    "    df[f'digit_freq_{digits[i]}'] = digit_freq[:, i]\n",
    "for i in range(len(punctuation)):\n",
    "    df[f'punctuation_freq_{punctuation[i]}'] = punctuation_freq[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae63bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "df['text'] = df['text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8b35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features that are not needed\n",
    "df = df.drop(columns=['gender', 'age', 'topic', 'sign', 'date'])\n",
    "\n",
    "# Split the dataset into training and testing sets by author\n",
    "authors = df['id'].unique()\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(authors)\n",
    "\n",
    "train_size = 0.8\n",
    "train_authors = authors[:int(len(authors) * train_size)]\n",
    "test_authors = authors[int(len(authors) * train_size):]\n",
    "\n",
    "train_df = df[df['id'].isin(train_authors)]\n",
    "test_df = df[df['id'].isin(test_authors)]\n",
    "\n",
    "# Save the training and testing sets\n",
    "train_df.to_csv('blogtext_train.csv', index=False)\n",
    "test_df.to_csv('blogtext_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca08c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get top 50 authors with the most posts\n",
    "# top_authors = df['id'].value_counts().nlargest(50).index\n",
    "\n",
    "# # Only keep the posts from the top 50 authors\n",
    "# df_50 = df[df['id'].isin(top_authors)]\n",
    "\n",
    "# # Create training and testing sets for the top 50 authors\n",
    "# # Shuffle the dataframe\n",
    "# df_50 = df_50.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# # Split the dataset into training and testing sets\n",
    "# train_size = 0.8\n",
    "# train_df_50 = df_50[:int(len(df_50) * train_size)]\n",
    "# test_df_50 = df_50[int(len(df_50) * train_size):]\n",
    "\n",
    "# # Save the training and testing sets\n",
    "# train_df_50.to_csv('blogtext_train_50.csv', index=False)\n",
    "# test_df_50.to_csv('blogtext_test_50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8604147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mean_word_len, std_word_len)\n",
    "# print(mean_sentence_len, std_sentence_len)\n",
    "# print(mean_short_words, std_short_words)\n",
    "# print(mean_hapax_legomena, std_hapax_legomena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa4ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
